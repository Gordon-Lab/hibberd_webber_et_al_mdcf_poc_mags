#Binning contigs based on Kallisto-calculated abundances 

#First, sync the anonymous contigs into a common directory:

###############################################################################################

#Generate Kallisto index for anonymous contigs

###############################################################################################

#!/bin/bash
#SBATCH --cpus-per-task=24
#SBATCH --mail-type=BEGIN,END,FAIL,REQUEUE
#SBATCH --mem=50G

ml kallisto/0.43.0

kallisto index \
-i kalliso_CAMI_anonymous_contigs_index \
/scratch/jglab/hmbucklin/03292021_abundances_for_anonymous_contigs/anonymous_gsa_pooled.fasta

###############################################################################################

#Quantify contig abundances across samples with Kallisto

###############################################################################################

#!/bin/bash
#SBATCH --cpus-per-task=24
#SBATCH --mail-type=BEGIN,END,FAIL,REQUEUE
#SBATCH --mem=100G
#SBATCH --array=0-63

ml kallisto/0.43.0

kallisto quant \
-i kalliso_CAMI_anonymous_contigs_index \
-b 100 \
-o sample_"${SLURM_ARRAY_TASK_ID}" \
/HL_working_directory/02092021_CAMI_mouse_data_resync/19122017_mousegut_scaffolds/2017.12.29_11.37.26_sample_"${SLURM_ARRAY_TASK_ID}"/reads/sample_"${SLURM_ARRAY_TASK_ID}"_R1_reads.fq \
/HL_working_directory/02092021_CAMI_mouse_data_resync/19122017_mousegut_scaffolds/2017.12.29_11.37.26_sample_"${SLURM_ARRAY_TASK_ID}"/reads/sample_"${SLURM_ARRAY_TASK_ID}"_R2_reads.fq

################################################################################################

#Sync abundances by sample name

################################################################################################

for i in {0..63}
do

SAMPLE_NAME=sample_"${i}"
echo $SAMPLE_NAME

rsync /HL_working_directory/03292021_abundances_for_anonymous_contigs/"${SAMPLE_NAME}"/abundance.tsv /HL_working_directory/03292021_abundances_for_anonymous_contigs/all_samples_kallisto/"${SAMPLE_NAME}"_abundance.tsv

done


for i in {0..63}
do

SAMPLE_NAME=sample_"${i}"
echo $SAMPLE_NAME

rsync /HL_working_directory/03292021_abundances_for_anonymous_contigs/"${SAMPLE_NAME}"/abundance.h5 /HL_working_directory/03292021_abundances_for_anonymous_contigs/"${SAMPLE_NAME}"/"${SAMPLE_NAME}"_abundance.h5

done


rsync /HL_working_directory/03292021_abundances_for_anonymous_contigs/sample_*/sample_*.h5 /HL_working_directory/03292021_abundances_for_anonymous_contigs/all_samples_h5_from_kallisto/

##################################################################################################

Generate contig depth table across all samples (kallisto_quantification_depth_table_generation.R)

##################################################################################################

####Code for kallisto_quantification_depth_table_generation.R######

library(progress)
library(rhdf5)
library(matrixStats)
library(dplyr)
library(tibble)

setwd("/HL_working_directory/03292021_abundances_for_anonymous_contigs/all_samples_h5_from_kallisto")

tpm <- function(counts, effLen) {
  rate <- log(counts) - log(effLen)
  denom <- log(sum(exp(rate)))
  exp(rate - denom + log(1E6))
}

#subgroup_file <- read.table("subgroup_file.txt", sep = "\t")
#colnames(subgroup_file) <- c("SID", "PID")

#participants <- c("sample_0", "sample_1", "sample_2", "sample_3", "sample_4", "sample_5", "sample_6", "sample_7", "sample_8", "sample_9")

#pb <- progress_bar$new(format = "[:bar] :current/:total (:percent) :eta", total = 952)
#for (i in 1:length(participants)) {

files <- list.files(".", "*.h5", full.names = TRUE, recursive = TRUE)

SID_array <- NA

res <- data.frame()

for (j in 1:length(files)) {
  #    pb$tick()
  SID <- gsub(".h5$", "", basename(files[j]))
  if (is.na(SID_array[1])) {
    SID_array <- SID
  } else {
    SID_array <- c(SID_array, SID)
  }
  boot_array <- h5read(files[j], "bootstrap")
  df_boot <- data.frame(contigName = h5read(files[j], "aux/ids"),
                        contigLen = h5read(files[j], "/aux/lengths"),
                        contigEffLen = h5read(files[j], "/aux/eff_lengths"))
  
  if (length(res) == 0) {
    res <- df_boot %>%
      select(-contigEffLen)
  }
  
  for (k in 1:length(boot_array)) {
    df_boot <- df_boot %>%
      bind_cols(data.frame(tpm(boot_array[[k]], df_boot$contigEffLen)))
    #%>% rename_with(function(x) paste0("bs", k - 1)))
    colnames(df_boot)[k+3] <- paste0("bs", k-1)
  }
  
  res_kallisto <- read.table(list.files(".", paste0(SID, ".tsv"), full.names = TRUE, recursive = TRUE), sep = "\t", header = TRUE)
  
  if (!all(df_boot$contigName == res_kallisto$target_id)) {
    stop()
  }
  
  res_sub <- data.frame(ab = res_kallisto$tpm,
                        #                          mean_boot = rowMeans(df_boot %>% select(-contigName, -contigLen, -contigEffLen)),
                        var = rowVars(as.matrix(df_boot %>% select(-contigName, -contigLen, -contigEffLen))),
                        contigName = df_boot$contigName,
                        contigLen = df_boot$contigLen)
  colnames(res_sub)[colnames(res_sub) == "ab"] <- SID
  colnames(res_sub)[colnames(res_sub) == "var"] <- paste0(SID, "-var")
  
  res <- res %>%
    full_join(res_sub, by = c("contigName", "contigLen"))
  
}

# res <- res %>%
#   select(SID_array) %>%
#   transmute(totalAvgDepth = rowSums(.)) %>%
#   bind_cols(res) %>%
#   relocate(totalAvgDepth, .after = contigLen)

all_depths <- res %>%
  select(all_of(SID_array))
totalAvgDepth <- rowSums(all_depths)
res_final <- add_column(res, totalAvgDepth, .after="contigLen")

write.table(res_final, paste0("CAMI_mouse_to_anonymous_contigs_kallisto.depth"), sep = "\t", quote = F, row.names = F)

###############################################################################################################################

#Bin contigs using MetaBAT2

################################################################################################################################

#!/bin/bash
#SBATCH --mem=250G
#SBATCH --cpus-per-task=12
#SBATCH --mail-type=END,FAIL,REQUEUE


ml metabat/2.12.1

#Then, can run MetaBat2:

metabat2 \
-i /HL_working_directory/03292021_abundances_for_anonymous_contigs/anonymous_gsa_pooled.fasta \
-a /HL_working_directory/03292021_abundances_for_anonymous_contigs/all_samples_h5_from_kallisto/CAMI_mouse_to_anonymous_contigs_kallisto.depth \
-o /HL_working_directory/03292021_abundances_for_anonymous_contigs/all_samples_h5_from_kallisto/MetaBAT_kallisto \
-v

################################################################################################################################

#Next, generate a list of the depths for each sample (kallisto_depth_file_to_list.py)

################################################################################################################################

######Code for kallisto_depth_file_to_list.py#########################################################

#!/usr/bin/env python3

import sys
import os
import errno
import argparse
import hashlib
import subprocess
import shutil
import re

parser = argparse.ArgumentParser()
parser.add_argument("-i", help="[STRING] Input depth file", dest="input", required=True)
parser.add_argument("-p", help="[STRING] Output prefix", dest="prefix", default = ".")
args = parser.parse_args()

args.input = args.input.strip()
args.prefix = args.prefix.strip()

print(args.input)
print(args.prefix)

dat = dict()
lines = open( args.input, 'r')
first = True
for line in lines:
    stripline = line.strip()
    splitline = stripline.split('\t')
    if (first):
        headers = splitline
        for i,e in enumerate(headers):
            if headers[i] == "contigName" or headers[i].startswith("P"):
                dat[headers[i]] = []

        first = False
    else:
        line_array = splitline
        for i,e in enumerate(line_array):
            if headers[i] == "contigName" or headers[i].startswith("P"):
                dat[headers[i]].append(line_array[i])

output_list = open(args.prefix + "/" + re.sub('\.depth', '', os.path.basename(args.input)) + ".depth_list", 'w')
for key in dat:
#    sys.stdout.write("{}\n".format(key))

 if key != "contigName":
        base = re.sub('abundance', '', key)
        outfile = args.prefix + "/" + base + ".depth"
        output = open(outfile, 'w')
        for i,e in enumerate(dat[key]):
            output.write("{}\t{}\n".format(dat["contigName"][i], dat[key][i]))
        output.close()
        output_list.write("{}\n".format(outfile))

output_list.close()

###################################################################################################################################
#Command to run kallisto_depth_file_to_list.py:

kallisto_depth_file_to_list.py -i /HL_working_directory/03292021_abundances_for_anonymous_contigs/all_samples_h5_from_kallisto/CAMI_mouse_to_anonymous_contigs_kallisto.depth -p /HL_working_directory/03292021_abundances_for_anonymous_contigs/all_samples_h5_from_kallisto

###################################################################################################################################

#Bin contigs using MaxBin2

###################################################################################################################################


#!/bin/bash
#SBATCH --mem=250G
#SBATCH --cpus-per-task=12
#SBATCH --mail-type=END,FAIL,REQUEUE

#ml maxbin/2.2.7
ml perl/5.20.3
ml perl-modules/5.20.3

export PATH=$PATH:/HL_working_directory/MaxBin/2.2.7/

perl /HL_working_directory/MaxBin/2.2.7/run_MaxBin.pl  \
        -contig /HL_working_directory/03292021_abundances_for_anonymous_contigs/anonymous_gsa_pooled.fasta \
        -out /HL_working_directory/03292021_abundances_for_anonymous_contigs/all_samples_h5_from_kallisto/MaxBin_kallisto \
        -abund_list /HL_working_directory/03292021_abundances_for_anonymous_contigs/all_samples_h5_from_kallisto/CAMI_mouse_to_anonymous_contigs_kallisto.depth_list \
        -thread 24

####################################################################################################################################

#Cut large contigs into smaller pieces

####################################################################################################################################

cut_up_fasta.py /HL_working_directory/03292021_abundances_for_anonymous_contigs/anonymous_gsa_pooled.fasta -c 10000 -o 0 --merge_last -b contigs_10K.bed > contigs_10K.fa

####################################################################################################################################

#Generate Kallisto index from output of cut_up_fasta.py

####################################################################################################################################

#!/bin/bash
#SBATCH --cpus-per-task=24
#SBATCH --mail-type=BEGIN,END,FAIL,REQUEUE
#SBATCH --mem=250G

ml kallisto/0.43.0

kallisto index \
-i kallisto_concoct_index \
/HL_working_directory/03292021_abundances_for_anonymous_contigs/bowtie_counts/contigs_10K.fa

####################################################################################################################################

#Quantify cut up contig abundances with Kallisto

####################################################################################################################################


#!/bin/bash
#SBATCH --cpus-per-task=24
#SBATCH --mail-type=BEGIN,END,FAIL,REQUEUE
#SBATCH --mem=100G
#SBATCH --array=0-63

ml kallisto/0.43.0

kallisto quant \
-i kallisto_concoct_index \
-b 100 \
-o sample_"${SLURM_ARRAY_TASK_ID}" \
/HL_working_directory/02092021_CAMI_mouse_data_resync/19122017_mousegut_scaffolds/2017.12.29_11.37.26_sample_"${SLURM_ARRAY_TASK_ID}"/reads/sample_"${SLURM_ARRAY_TASK_ID}"_R1_reads.fq \
/HL_working_directory/02092021_CAMI_mouse_data_resync/19122017_mousegut_scaffolds/2017.12.29_11.37.26_sample_"${SLURM_ARRAY_TASK_ID}"/reads/sample_"${SLURM_ARRAY_TASK_ID}"_R2_reads.fq

################################################################################################################################

#Rename Kallisto output files

################################################################################################################################

for i in {0..63}
do

SAMPLE_NAME=sample_"${i}"
echo $SAMPLE_NAME

rsync /HL_working_directory/03292021_abundances_for_anonymous_contigs/kallisto_for_concoct/"${SAMPLE_NAME}"/abundance.tsv /HL_working_directory/03292021_abundances_for_anonymous_contigs/kallisto_for_concoct/all_files_for_coverage/"${SAMPLE_NAME}"_abundance.tsv

done


for i in {0..63}
do

SAMPLE_NAME=sample_"${i}"
echo $SAMPLE_NAME

rsync /HL_working_directory/03292021_abundances_for_anonymous_contigs/kallisto_for_concoct/"${SAMPLE_NAME}"/abundance.h5 /HL_working_directory/03292021_abundances_for_anonymous_contigs/kallisto_for_concoct/all_files_for_coverage/"${SAMPLE_NAME}"_abundance.h5

done

################################################################################################################################

#Generate CONCOCT contig coverage table (CONCOCT_contig_coverage_table_generation.R)

################################################################################################################################
#Code for CONCOCT_contig_coverage_table_generation.R

library(progress)
library(rhdf5)
library(matrixStats)
library(dplyr)
library(tibble)

setwd("/HL_working_directory/03292021_abundances_for_anonymous_contigs/kallisto_for_concoct/all_files_for_coverage")

tpm <- function(counts, effLen) {
  rate <- log(counts) - log(effLen)
  denom <- log(sum(exp(rate)))
  exp(rate - denom + log(1E6))
}

#subgroup_file <- read.table("subgroup_file.txt", sep = "\t")
#colnames(subgroup_file) <- c("SID", "PID")

#participants <- c("sample_0", "sample_1", "sample_2", "sample_3", "sample_4", "sample_5", "sample_6", "sample_7", "sample_8", "sample_9")

#pb <- progress_bar$new(format = "[:bar] :current/:total (:percent) :eta", total = 952)
#for (i in 1:length(participants)) {

files <- list.files(".", "*.h5", full.names = TRUE, recursive = TRUE)

SID_array <- NA

res <- data.frame()

for (j in 1:length(files)) {
  #    pb$tick()
  SID <- gsub(".h5$", "", basename(files[j]))
  if (is.na(SID_array[1])) {
     SID_array <- SID
  } else {
    SID_array <- c(SID_array, SID)
  }
  boot_array <- h5read(files[j], "bootstrap")
  df_boot <- data.frame(contigName = h5read(files[j], "aux/ids"),
                        contigLen = h5read(files[j], "/aux/lengths"),
                        contigEffLen = h5read(files[j], "/aux/eff_lengths"))

  if (length(res) == 0) {
    res <- df_boot %>%
      select(-contigEffLen)
  }

  for (k in 1:length(boot_array)) {
    df_boot <- df_boot %>%
      bind_cols(data.frame(tpm(boot_array[[k]], df_boot$contigEffLen)))
    #%>% rename_with(function(x) paste0("bs", k - 1)))
    colnames(df_boot)[k+3] <- paste0("bs", k-1)
  }

  res_kallisto <- read.table(list.files(".", paste0(SID, ".tsv"), full.names = TRUE, recursive = TRUE), sep = "\t", header = TRUE)

  if (!all(df_boot$contigName == res_kallisto$target_id)) {
    stop()
  }
  res_sub <- data.frame(ab = res_kallisto$tpm,
                        #                          mean_boot = rowMeans(df_boot %>% select(-contigName, -contigLen, -contigEffLen)),
                        #var = rowVars(as.matrix(df_boot %>% select(-contigName, -contigLen, -contigEffLen))),
                        contigName = df_boot$contigName,
                        contigLen = df_boot$contigLen)
  colnames(res_sub)[colnames(res_sub) == "ab"] <- paste0("cov_mean_sample_",SID)
  #colnames(res_sub)[colnames(res_sub) == "var"] <- paste0(SID, "-var")

  res <- res %>%
    full_join(res_sub, by = c("contigName", "contigLen"))

}
#all_depths <- res %>%
#  select(all_of(SID_array))
#totalAvgDepth <- rowSums(all_depths)
res_final <- res #add_column(res, totalAvgDepth, .after="contigLen")

res_final$contigLen <- NULL
#res_final$totalAvgDepth <- NULL
colnames(res_final)[1] <- c("contig")

write.table(res_final, paste0("CAMI_mouse_to_anonymous_contigs_kallisto_concoct.tsv"), sep = "\t", quote = F, row.names = F)

################################################################################################################################

#Bin contigs using CONCOCT

################################################################################################################################

#!/bin/bash
#SBATCH --cpus-per-task=24
#SBATCH --mail-type=BEGIN,END,FAIL,REQUEUE
#SBATCH --mem=150G

export PATH=$PATH:/HL_home_directory/.conda/envs/concoct_env/bin/

mkdir -p concoct_kallisto_output

#concoct_coverage_table.py contigs_10K.bed /HL_working_directory/03292021_abundances_for_anonymous_contigs/bowtie_counts/sample_*_nodup.bam > coverage_table.tsv
concoct --composition_file contigs_10K.fa --coverage_file CAMI_mouse_to_anonymous_contigs_kallisto_concoct.tsv -b concoct_kallisto_output/
merge_cutup_clustering.py concoct_kallisto_output/clustering_gt1000.csv > concoct_kallisto_output/clustering_kallisto_merged.csv


################################################################################################################################

#Dereplicate binned contigs across methods with DASTool

################################################################################################################################

#First create the scaffolds2bin files

################################################################################################################################


#!/bin/bash
#SBATCH --mem=50G
#SBATCH --cpus-per-task=12
#SBATCH --mail-type=BEGIN,END,FAIL,REQUEUE

ml das_tool/1.1.2

rsync /HL_working_directory/03292021_abundances_for_anonymous_contigs/Fasta_to_Scaffolds2Bin.sh /HL_working_directory/03292021_abundances_for_anonymous_contigs/bowtie_counts/MetaBAT_bowtie/

./Fasta_to_Scaffolds2Bin.sh \
-i /HL_working_directory/03292021_abundances_for_anonymous_contigs/all_samples_h5_from_kallisto/MaxBin_kallisto \
-e "fasta" \
> /HL_working_directory/03292021_abundances_for_anonymous_contigs/all_samples_h5_from_kallisto/MaxBin_kallisto/kallisto_maxbin_bins_scaffolds2bin.tsv

################################################################################################################################

#!/bin/bash
#SBATCH --mem=50G
#SBATCH --cpus-per-task=12
#SBATCH --mail-type=BEGIN,END,FAIL,REQUEUE

ml das_tool/1.1.2

rsync /HL_working_directory/03292021_abundances_for_anonymous_contigs/Fasta_to_Scaffolds2Bin.sh /HL_working_directory/03292021_abundances_for_anonymous_contigs/all_samples_h5_from_kallisto/MetaBAT_kallisto/

./Fasta_to_Scaffolds2Bin.sh \
-i /HL_working_directory/03292021_abundances_for_anonymous_contigs/all_samples_h5_from_kallisto/MetaBAT_kallisto \
-e "fa" \
> /HL_working_directory/03292021_abundances_for_anonymous_contigs/all_samples_h5_from_kallisto/MetaBAT_kallisto/kallisto_metabat_bins_scaffolds2bin.tsv

####################################################################################################################################

#Obtained scaffolds2bin information for CONCOCT from clustering_kallisto_merged.csv file

######################################################################################################################################

#Next, dereplicate bins across methods with DASTool

######################################################################################################################################

#!/bin/bash
#SBATCH --mem=100G
#SBATCH --cpus-per-task=12
#SBATCH --mail-type=END,FAIL,REQUEUE

export PATH=$PATH:/HL_working_directory/DASTool_trial/1.1.2
export PATH=$PATH:/HL_working_directory/ruby/ruby-2.6.2
export PATH=$PATH:/HL_working_directory/03292021_abundances_for_anonymous_contigs/DASTool_new_kallisto/usearch

#export PATH=$PATH:/HL_working_directory/usearch
#ml das_tool/1.1.2
#ml usearch/10.0.240
#ml usearch/11.0.667
ml R/3.6.1
#ml ruby/2.6.2
ml pullseq/1.0.2
ml prodigal/2.6.3
ml diamond/0.9.34-python-3.6.5

DAS_Tool \
-i concoct_kallisto_contigs_to_scaffolds.tsv,kallisto_maxbin_bins_scaffolds2bin.tsv,kallisto_metabat_bins_scaffolds2bin.tsv \
-l CONCOCT,MaxBin2,MetaBAT2 \
-c /scratch/jglab/hmbucklin/03292021_abundances_for_anonymous_contigs/anonymous_gsa_pooled.fasta \
-o kallisto_maxbin_metabat_concoct \
--write_bins 1 \
--search_engine diamond

######################################################################################################################################
